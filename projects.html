---
layout: default
---

<!--
	Twenty by HTML5 UPs
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->

		<!-- Main -->
			<article id="main">
				<section id="cta">
					<header class="special container">
						<span class="icon fa-space-shuttle"></span>
						<h2>Projects</h2>
						<p>Here are some details on a couple of the projects I spend my free time on.</p>
						<p></p>
						<ul class="buttons">
							<div>
								<li><a href="#bayes_nns" class="button fit scrolly">Bayesian neural networks</a></li>
							</div>
							<p></p>
							<div>
								<li><a href="#marg_like" class="button fit scrolly">Estimating marginal likelihoods with deep learning</a></li>
							</div>
						</ul>
					</header>
				</section>
				<section class="wrapper style4 container" id="bayes_nns">

						<div class="row oneandhalf">
							<div class="4u">

								<!-- Sidebar -->
									<div class="sidebar">
										<section>
											<header>
												<h3><strong>Bayesian neural networks</strong></h3>
											</header>
										</section>
										<section>
											<p><a href="#main" class="button small scrolly">Back to top</a> </p>
										</section>
									</div>

							</div>
							<div class="8u skel-cell-important">

									<!-- Content -->
										<div class="content">
											<section>
												<p>
													<strong>Github repositiory: <a href="https://github.com/gregoryross/langevin_neural_nets">Langevin neural nets</a> </strong><br/>
													I find Bayesian inference very appealing because it forces us to confront
													what we know and do not know about the phenomena we are trying to model.
													Fortunately for me there is a one-to-one correspondence between molecular dynamics simulations
													and Bayesian inference. The same computational methods that are used to
													sample molecular configurations can be directly applied to sample
													the parameters of Bayesian models.
												</p>
												<p>
													Neural networks are very flexible function approximators and, when
													combined with Bayesian methods, offer the possiblity of performing
													robust inference on almost any problem. Like molecular simulations,
													neural nets can have an extremely large number of parameters that can
													be difficult to infer. This project is about applying the kinds of sampling
													methods I'm used to developing and using on molecular dynamics to neural nets.
												</p>
											</section>
										</div>
							</div>
						</div>
				</section>
				<section class="wrapper style4 container" id="marg_like">

						<div class="row oneandhalf">
							<div class="4u">

								<!-- Sidebar -->
									<div class="sidebar">
										<section>
											<header>
												<h3><strong>Estimating marginal likelihoods with deep learning</strong></h3>
											</header>
										</section>
										<section>
											<p><a href="#main" class="button small scrolly">Back to top</a> </p>
										</section>
									</div>

							</div>
							<div class="8u skel-cell-important">

									<!-- Content -->
										<div class="content">
											<section>
												<p>
													<strong>Github repositiory: <a href="https://github.com/gregoryross/deep_marginal_likelihoods">Deep marginal likelihoods</a> </strong><br/>
													When attempting to create quantitative and predictive models of data,
													one seldom has a single model in mind. At some point, one inevitably
													has to select aspects of the model like the functional form and the number of parameters.
													The formal approach to this choice is known as model selection, which can be an immensely
													complex and difficult task to do rigorously. The singular interest of this project is model selection in a Bayesian context,
													whose framework naturally admits a quantity - the marginal likelihood or marginal evidence -
													through which all models can be judged and compared.
												</p>
												<p>
													In this project, I've derived an upper bound to the marginal likelihood
													that can be computed with deep learning methods. Bayesian analysis
													and machine learning are often (wrongly) considered to be distinct approaches,
													and, in this project, it's been fun to make to connect the two fields
													so explicitly. My <a href="https://github.com/gregoryross/deep_marginal_likelihoods">prototyped codes</a>
													provides validation of my analytical results.
												</p>
												<p>
													A huge part of my research centers around free energy calculations
													in molecular simulations, which has a direct correspondence with
													Bayesian model seletion. This project draws on some of this
													experience.
												</p>
											</section>
										</div>
							</div>
						</div>
				</section>

			</article>
